<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>EECS 484 Final Cheating Paper</title>
    <link rel="stylesheet" href="css/html5reset.css">
    <link rel="stylesheet" href="css/style.css">
    <link rel="stylesheet" href="css/imgs.css">
    <script src="script.js"></script>
</head>

<!-- 
Slotted Pages
Index Matching
Compare Indexing Methods
Static Hashing Schemes
Extendible Hashing
Linear Hashing
B+ Tree Indexing
Join Algorithms
NSM vs. DSM
Sorting
Iterator vs. Vectorized vs. Materialized Model
Parallel Execution Process Models
Intra-Query Parallelism
Serializability
Two-Phase Locking
Logging/Checkpoints
ARIES
ACID Properties
Deadlock Detection/Prevention
Isolation Levels
Heuristics-based Query Optimization
Cost-based Query Optimization -->

<body>
    <h1>EECS 484 Final Cheating Paper</h1>
    <div id="main">
        <h2 id="name">Tiancheng Jiao&nbsp;|&nbsp;tcjiao&nbsp;|&nbsp;EECS 484</h2>
        <div class="line"></div>
        <div class="text-img-25">
            <div>
                <h2>Tuple Storage</h2>
                <div class="st"><p>Strawman Idea:</p> Keep track of the
                    number of tuples in a page and then
                    just append a new tuple to the end.
                    中间允许有空隙</div>
                <p class="spst">Slotted Pages</p>
                <p>The slot array maps "slots" to the
                    tuples' starting position offsets.</p>
                <p>The header keeps track of:
                    the # of used slots,
                    the offset of the starting location of the last slot used. </p>
            </div>
            <img src="imgs/sp.png">
        </div>
        <p>需要注意所有的实际tuple必须集中在最后且连续，即删除后如果有空隙需要后移</p>
        <p>Record IDs: page_id + offset/slot</p>
        <div class="st"><p>OLTP: </p>On-line Transaction Processing, small, 写入更多</div>
        <div class="st"><p>OLAP: </p>On-line Transaction Processing, big, 读取更多</div>
        <p>Hybrid Transaction + Analytical Processing: OLTP + OLAP</p>
        <div class="st"><p>NSM: </p>N-ARY Storage Model, 按行储存</div>
        <div class="st"><p>DSM: </p>Decomposition Storage Model, 按列储存</div>
        <div class="line"></div>
        <h2>Hashing</h2>
        <div class="st"><p>Static Hash Table: </p>Allocate a giant array that has one
            slot for every element you need to
            store. <p>Assume</p> if key1≠key2, then
            hash(key1)≠hash(key2). 即事先分配好，且不可重复。</div>
        <div class="st"><p>Linear Probe: </p>重复了就放到下一个，删除了需要标记或移动</div>
        <div class="text-img-38">
            <div>
                <div class="st"><p>Non-Unique Keys</p></div>
                <p>Choice #1: Separate Linked List
                    → Store values in separate storage area for
                    each key.</p>
                <p>Choice #2: Redundant Keys
                    → Store duplicate keys entries together in
                    the hash table.
                    → This is easier to implement so this is
                    what most systems do.</p>
                <br><br>
                <div class="st"><p>Robin Hood Hashing: </p>Variant of linear probe hashing that steals slots
                    from "rich" keys and give them to "poor" keys.
                    </div>
            </div>
            <img src="imgs/nuk.png">
        </div>
        <p>→ Each key tracks the number of positions they are from
            where its optimal position in the table.<br>
            → On insert, a key takes the slot of another key if the first
            key is farther away from its optimal position than the
            second key.</p>
        <p>在插入时记录实际位置和哈希位置的偏移量，多个key抢占同一位置时，哈希值最小的优先。
            如果相等，不会移动已经在那里的。
        </p>
        <div class="st"><p>Cuckoo Hashing: </p>Use multiple hash tables with different hash
            function seeds.<br>
            → On insert, check every table(计算多次哈希) and pick anyone that has a
            free slot.<br>
            → If no table has a free slot, evict(删除) the element from one
            of them and then re-hash it find a new location.<br>
            O(1) in find and delete, only check one location.<br>
            Need to first know the element num. Rebuild when grow/shrink.
        </div>
        <div class="st"><p>Chained Hashing: </p>
            Maintain a linked list of buckets for each slot in
            the hash table.<br>
            Resolve collisions by placing all elements with
            the same hash key into the same bucket. <br>
            → To determine whether an element is present, hash to
            its bucket and scan for it.<br>
            → Insertions and deletions are generalizations of lookups.<br>
            如果满了要超出就再创建一个，链接过去，类似于linked list，但是bucket大小超过1。
        </div>
        <div class="st"><p>Extendible Hashing: </p>Chained-hashing approach where we split
            buckets instead of letting the linked list grow
            forever.<br>
            Data movement is localized to just the split chain<br>
            → Multiple slot locations can point to the same bucket
            chain. <br>
            → Reshuffle bucket entries on split and increase the
            number of bits to examine. <br>
            每个bucket固定大小，使用二进制的prefix matching，如果满了要超出就把那个prefix的拆成两个。
            prefix长度可以不同，即只拆超出的那个，别的不拆。维护一个global变量来记录最大的prefix长度。
            每个page也有一个local变量来记录自己的prefix长度。
        </div>
        <div class="text-img-35">
            <div>
                <div class="st"><p>Linear Hashing: </p>
                    The hash table maintains a pointer that tracks
                    the next bucket to split.<br>
                    → When any bucket overflows, split the bucket at the
                    pointer location.<br>
                    Use multiple hashes to find the right bucket for a
                    given key.<br>
                    Can use different overflow criterion:
                    → Space Utilization
                    → Average Length of Overflow Chains
                </div>
            </div>
            <img src="imgs/lh.png">
        </div>
        <p>只要插入那个bucket后，那个bucket有overflow，就要split，即使插入行为并没有引发overflow。(这是hw4里的要求)
            课件上是只要有任何一个overflow就split，具体看题目要求。一定要注意overflow描述的是状态，不是行为。</p>
        <p>有一个next指针，指向下一个要拆分的bucket，每次拆分后next指针后移。从尾部到开头之后，level加1。
            level=floor(log2(bucket_num))-1，即正在使用的哈希函数的位数的最小值。N是初始的bucket数量。
            正常都需要两个哈希函数，只有next正好指向0时，只需要一个(但也可以把比它小的保留着)。
            h0有1位，h1有2位，以此类推。当前循环中未被拆分的使用小的哈希函数查找，拆分了的使用大的。
            有overflow就使用linked list。初始状态level=0，两个bucket。
            每次插入之后，根据条件判断是否要split，要就拆分next指针指向的那个。
        </p>
        <p class="spst">注意以上所有方式都需要保存key</p>
        <div class="line"></div>
        <h2>B+ Trees</h2>
        <p>O(log n) searches, insertions, and deletions</p>
        <div class="st">A B+Tree is an <p>M-way</p> search tree with the
            following properties <p>(需要注意M是格子数+1)</p>: <br>
            It is perfectly balanced (i.e., every leaf node is at the
            same depth in the tree)<br>
            → Every node other than the root is at least half-full
            M/2-1 ≤ #keys ≤ M-1 即至少占据floor(格子数/2)，最多占满<br>
            → Every inner node with k keys has k+1 non-null children
        </div>
        <img src="imgs/bpt.png">
        <div class="st"><p>Leaf node</p>: 最底层的，<p>inner node</p>: 除了最底层的(包括root)</div>
        <p>Leaf直接相邻的有箭头互相连接(双向)，是所有的leaf之间连接，不管parent是否相同。inner node间不需要。</p>
        <p>每一个箭头的范围是：大于等于左边值，严格小于右边值<br>
            或：左边箭头严格小于自己，右边箭头大于等于自己
        </p>
        <p>每个leaf node可以保存key value，也可以保存指针</p>
        <p class="useless">INSERT<br>
            If L has enough space, done!<br>
            Otherwise, (optionally, try to re-distribute a key
            to sibling nodes first) split L keys into L and a
            new node L2<br>
            → Redistribute entries evenly, copy up middle key.<br>
            → Insert index entry pointing to L2 into parent of L.<br>
            To split inner node, redistribute entries evenly,
            but push up middle key</p>
        <p class="useless">DELETE<br>
            If L is at least half-full, done!<br>
            If L has only M/2-1 entries,<br>
            → Try to re-distribute, borrowing from sibling (adjacent
            node with same parent as L).<br>
            → If re-distribution fails, merge L and sibling.<br>
            If merge occurred, must delete entry (pointing to
            L or sibling) from parent of L.<br>
        </p>
        <div class="st"><p>插入或删除: </p><br>
            先尝试是否能塞一个到左边或右边/借一个(需要是同一个parent)，可以就这样做，结束了。但是要注意新的索引值应该选取改变部分的新的最小值(即应该尽可能选取已有的值)，而不是在范围内随机选择。<br>
            在没有塞/借/拆分/合并/rebuild的情况下，不可以动索引的值。整体移动也不可以改索引的值。<br>
            在可以的情况下，把一个node拆成两个/两个合并成一个。不要动无关的索引值，并且如果是insert，新的索引值应该选用右边的最小值，而不是随便选一个。<br>
            如果以上都不行，就要rebuild，同样需要遵循尽可能不动索引值、优先选择已有的值。
        </div>
        <div class="st"><p>Duplicate Keys: </p><br>
            Approach #1: Append Record ID<br>
            → Add the tuple's unique Record ID as part of the key to
            ensure that all keys are unique. (即同时使用原本的key和id共同作为key，一定唯一)<br>
            → The DBMS can still use partial keys to find tuples.<br>
            Approach #2: Overflow Leaf Nodes<br>
            → Allow leaf nodes to spill into overflow nodes that
            contain the duplicate keys.<br>
            → This is more complex to maintain and modify.
        </div>
        <div class="line"></div>
        <h2>Sorting</h2>
        <p class="spst">2-Way External Merge Sort</p>
        <p>Pass #0<br>
            → Read pages of the table into memory, one at a time<br>
            → Sort the page into a run and write it back to disk<br>
            Pass #1,2,3,…<br>
            → Recursively merge pairs of runs into runs twice as long<br>
            → Uses three buffer pages (2 for input pages, 1 for output)<br>
            Number of passes = 1 + ceil(log2 N)<br>
            Total I/O cost = 2N * (# of passes)
            需要3个buffer
        </p>
        <p class="spst">有B个buffer</p>
        <p>Pass #0<br>
            → Assume the DBMS has a finite number of B buffer pool
            pages to hold input and output data.<br>
            → Read B pages of the table into memory at a time. Sort
            them and write back to disk.<br>
            → Produce ⌈N / B⌉ sorted runs of size B<br>
            Pass #1,2,3,…<br>
            → Merge B-1 runs (i.e., K-way merge)<br>
            Number of passes = 1 + ceil[log(B-1) ceil(N/B)]<br>
            Total I/O Cost = 2N ∙ (# of passes)
        </p>
        <p class="spst">Aggregations (执行 Select Distinct)</p>
        <p>两种方案：sorting, hashing<br>
            sorting: 排序之后，顺序读取，去除重复<br>
            hasing(无需排序的情况, Group By, Distinct): 速度更快，<br>
            Phase #1 – Partition<br>
            Use a hash function h1
            to split tuples into
            partitions on disk.<br>
            → A partition is one or more pages that contain the set of
            keys with the same hash value.<br>
            → Partitions are “spilled” to disk via output buffers.<br>
            Assume that we have B buffers.<br>
            We will use B-1 buffers for the partitions and 1
            buffer for the input data.<br>
            Phase #2 – ReHash<br>
            For each partition on disk:<br>
            → Read it into memory and build an in-memory hash
            table based on a second hash function h2.<br>
            → Then go through each bucket of this hash table to bring
            together matching tuples.<br>
            This assumes that each partition fits in memory.<br>
            During the ReHash phase, store pairs of the form
            (GroupKey→RunningVal) (比如计算Sum Avg)<br>
            When we want to insert a new tuple into the
            hash table:<br>
            → If we find a matching GroupKey, just update the
            RunningVal appropriately<br>
            → Else insert a new GroupKey→RunningVal
        </p>
        <div class="line"></div>
        <h2>Join</h2>
        <p>注意以下全部不考虑输出的写入量</p>
        <p class="spst">Stupid Nested Loop Join</p>
        <p>foreach tuple r ∈ R: // outer<br>
            &nbsp;&nbsp;foreach tuple s ∈ S: // inner<br>
            &nbsp;&nbsp;&nbsp;&nbsp;emit, if r and s match<br>
            M: R的页数，m: R的record数，N: S的页数，n: S的record数<br>
            I/O: min(M+(m*N), N+(n*M))
        </p>
        <p class="spst">Block Nested Loop Join</p>
        <p>
            <pre>
foreach (B - 2) blocks b(R) ∈ R:
    foreach block b(S) ∈ S:
        foreach tuple r ∈ (B - 2) blocks:
            foreach tuple s ∈ b(S):
                emit, if r and s match
I/O: M+ceil(M/(B-2))*N 或 M+N (若B>M+2/N+2)</pre>
        </p>
        <p class="spst">Index Nested Loop Join</p>
        <p>
            foreach tuple r in R do<br>
            &nbsp;&nbsp;foreach tuple s in S where (index of r) == (index of s) do<br>
            &nbsp;&nbsp;&nbsp;&nbsp;emit, if r and s match<br>
            其中s的index是通过B+Tree在整个里面查找，不是遍历<br>
            Cost: M+(m*C) (C是常数，在B+Tree中查找的时间)<br>
            Pick the smaller table as the outer table.<br>
            Buffer as much of the outer table in memory as
            possible.<br>
            Loop over the inner table (or use an index).
        </p>
        <p class="spst">Sort-Merge Join</p>
        <p>
            sort R,S on join keys<br>
            cursor(R) ← R(sorted), cursor(S) ← S(sorted)<br>
            while cursor(R) and cursor(S):<br>
            &nbsp;&nbsp;if cursor(R) > cursor(S):<br>
            &nbsp;&nbsp;&nbsp;&nbsp;increment cursor(S)<br>
            &nbsp;&nbsp;if cursor(R) < cursor(S):<br>
            &nbsp;&nbsp;&nbsp;&nbsp;increment cursor(R)<br>
            &nbsp;&nbsp;elif cursor(R) and cursor(S) match:<br>
            &nbsp;&nbsp;&nbsp;&nbsp;emit<br>
            &nbsp;&nbsp;&nbsp;&nbsp;increment cursor(S)<br>
            Sort Cost (R): 2M*(1+ceil(log(B-1) ceil(M/B)))<br>
            Sort Cost (S): 2N*(1+ceil(log(B-1) ceil(N/B)))<br>
            Merge Cost: (M + N)<br>
            Total Cost: Sort + Merge
        </p>
        <p class="spst">Basic Hash Join</p>
        <p>
            build hash table HT(R) for R<br>
            &nbsp;&nbsp;foreach tuple s ∈ S<br>
            &nbsp;&nbsp;&nbsp;&nbsp;output, if h1(s) ∈ HT(R)<br>
            Can use full tuple or tuple identifier.<br>
            Problem: do not have enough
            memory to fit the entire hash table
        </p>
        <div class="text-img-55">
            <div>
                <p class="spst">Grace Hash Join</p>
                <p>
                    Build Phase: Hash both tables on the
                    join attribute into partitions.<br>
                    Probe Phase: Compares tuples in
                    corresponding partitions for each table.
                </p>
                <p>第一次hash有B-1个bucket(需要存放读取的值)，第二次hash有B-2个(需要存放S读取的值和结果)。</p>
                <p>Cost: 3(M+N) (1R1W in partitioning, 1R in probing)</p>
                <div class="light-line"></div>
                <p>How big of a table can we hash using this
                    approach?<br>
                    Answer: B*(B-1), A table of N pages needs about sqrt(N) buffers<br>
                    这里先不考虑B*(B-1)还是(B-1)*(B-2)的问题
                </p>
                <p>可以recursive partitioning如果内存还是不够</p>
            </div>
            <img src="imgs/ghj.png">
        </div>
        <div class="line"></div>
        <h2>Query Execution</h2>
        <div class="st"><p>Materialization</p><br>
            Early Materialization:
            Copy the values for the attributes in
            outer and inner tuples into a new output
            tuple.<br>
            Late Materialization:
            Only copy the joins keys along with the
            Record IDs of the matching tuples.
        </div>
        <p class="spst">PROCESSING MODELS</p>
        <div class="st">
            <p>Approach #1: Iterator Model</p><br>
            Each query plan operator implements a Next()
            function.<br>
            → On each invocation, the operator returns either a
            single tuple or a null marker if there are no more
            tuples.<br>
            → The operator implements a loop that calls Next() on
            its children to retrieve their tuples and then process
            them.<br>
            Also called Volcano or Pipeline Model.<br>
            <img src="imgs/im.png" width="100%"><br>
            需求分析是从上到下，实际执行是从下到上<br>
            Some operators must block until their children
            emit all their tuples.<br>
            <p>Approach #2: Materialization Model</p><br>
            就是每一步操作都要等到结束之后再返回，一次性返回完整的结果。<br>
            Each operator processes its input all at once and
            then emits its output all at once.<br>
            → The operator "materializes" its output as a single
            result.<br>
            → The DBMS can push down hints (e.g., LIMIT) to avoid
            scanning too many tuples.<br>
            → Can send either a materialized row or a single column.<br>
            The output can be either whole tuples (NSM) or
            subsets of columns (DSM).<br>
            Better for OLTP, Not good for OLAP<br>
            <div class="text-img-60">
                <div>
                    <p>Approach #3: Vectorization Model</p><br>
                    前面两种的结合<br>
                    Like the Iterator Model where each operator
                    implements a Next() function, but…<br>
                    Each operator emits a batch of tuples instead of
                    a single tuple.<br>
                    → The operator's internal loop processes multiple tuples
                    at a time.<br>
                    → The size of the batch can vary based on hardware or
                    query properties.
                </div>
                <img src="imgs/vm.png">
            </div>
        </div>
        <p class="spst">ACCESS METHODS</p>
        <div class="st"><p>1. Sequential Scan</p><br>
            For each page in the table:<br>
            → Retrieve it from the buffer pool.
            → Iterate over each tuple and check
            whether to include it.<br>
            Sequential Scan Optimizations:
            Prefetching,
            Buffer Pool Bypass,
            Parallelization,
            Heap Clustering,
            Zone Maps,
            Late Materialization<br>
            <p>Zone Maps: </p>
            Pre-computed aggregates for the attribute
            values in a page. DBMS checks the zone map first
            to decide whether it wants to access the page.
            就是提前计算好比如Min Max Sum Avg之类的，直接读取它判断要不要，不用再全部读取。也有比如Max只有15，但是要选取大于20的，就不用读取了。<br>
            <p>Late Materialization: </p><br>
            DSM DBMSs can delay stitching together tuples
            until the upper parts of the query plan.
            就是可以只先记录record id(offset)，每次取根据id读取，因为是DSM，根据属性读取是连续的。<br>
            <p>2. Index Scan</p><br>
            要提前根据所用的属性建立索引(比如B+Tree)，然后根据索引查找。<br>
            Select * From Students Where Age < 30 And Dept = 'CS'<br>
            Index 1: Age, Index 2: dept<br>
            <p>3. Multi-Index Scan</p><br>
            先根据多个index单独查找，找到对应的set，然后再union或intersect。<br>
            If there are multiple indexes that the DBMS can
            use for a query:<br>
            → Compute sets of Record IDs using each matching index.<br>
            → Combine these sets based on the query's predicates
            (union vs. intersect).<br>
            → Retrieve the records and apply any remaining
            predicates.
        </div>
        <p class="useless">
            HALLOWEEN PROBLEM<br>
            Anomaly where an update operation changes
            the physical location of a tuple, which causes a
            scan operator to visit the tuple multiple times.<br>
            → Can occur on clustered tables or index scans.<br>
            First discovered by IBM researchers while
            working on System R on Halloween day in 1976.<br>
            Solution: Track modified record ids per query.<br>
            EXPRESSION EVALUATION<br>
            The DBMS represents a WHERE clause
            as an expression tree.<br>
            The nodes in the tree represent
            different expression types:<br>
            → Comparisons (=, <, >, !=)<br>
            → Conjunction (AND), Disjunction (OR)<br>
            → Arithmetic Operators (+, -, *, /, %)<br>
            → Constant Values<br>
            → Tuple Attribute References<br>
            Evaluating predicates in this manner
            is slow.<br>
            → The DBMS traverses the tree and for
            each node that it visits it must figure out
            what the operator needs to do.<br>
            Consider this predicate:<br>
            WHERE S.val=1<br>
            A better approach is to just evaluate
            the expression directly.<br>
            → Think JIT compilation
        </p>
        <p class="spst">PROCESS MODEL</p>
        <div class="st"><p>1. Process(进程) Per Worker</p><br>
            Each worker is a separate OS process.<br>
            → Relies on OS scheduler.<br>
            → Use shared-memory for global data structures.<br>
            → A process crash does not take down entire system.<br>
            → Examples: IBM DB2, Postgres, Oracle<br>
            <p>2. Thread(线程) Per Worker</p><br>
            Single process with multiple worker threads.<br>
            → DBMS (mostly) manages its own scheduling.<br>
            → May or may not use a dispatcher thread.<br>
            → Thread crash (may) kill the entire system.(理解成总共就一个进程，哪里出问题了可能就要整个进程终止)<br>
            → Examples: MSSQL, MySQL, DB2, Oracle (2014)<br>
            <p>3. Embedded DBMS</p><br>
            DBMS runs inside of the same address space as
            the application. Application is (mostly)
            responsible for threads and scheduling.<br>
            The application may support outside
            connections.<br>
            → Examples: BerkeleyDB, SQLite, RocksDB, LevelDB
        </div>
        <p class="useless">
            SQLOS<br>
            SQLOS is a user-level OS layer that runs inside of
            the DBMS and manages provisioned hardware
            resources.<br>
            → Determines which tasks are scheduled onto which
            threads.<br>
            → Also manages I/O scheduling and higher-level concepts
            like logical database locks.<br>
            Non-preemptive thread scheduling through
            instrumented DBMS code.
        </p>
        <div class="st"><p>Inter-Query Parallelism: </p>
            Execute multiple disparate(不同的) queries
            simultaneously.
            → Increases throughput & reduces latency.</div>
        <div class="st"><p>Intra-Query Parallelism: </p>
            Execute the operations of a single
            query in parallel.
            → Decreases latency for long-running queries, especially
            for OLAP queries.</div>
        <div class="st"><p>Intra-Query Parallelism</p><br>
            <p>Approach #1: Intra-Operator (Horizontal)</p><br>
            Decompose operators into independent fragments
            that perform the same function on different subsets of
            data.<br>
            The DBMS inserts an exchange operator into the
            query plan to coalesce/split results from multiple
            children/parent operators.<br>
            <img src="imgs/iop.png" width="62%" style="display:inline"><img src="imgs/gdr.png" width="38%" style="display:inline"><br>
            Exchange Type #1 – Gather<br>
            → Combine the results from multiple
            workers into a single output stream.<br>
            Exchange Type #2 – Distribute<br>
            → Split a single input stream into multiple
            output streams.<br>
            Exchange Type #3 – Repartition<br>
            → Shuffle multiple input streams across
            multiple output streams.<br>
            <p>Approach #2: Inter-Operator (Vertical)</p><br>
            → Operations are overlapped in order to pipeline data
            from one stage to the next without materialization.<br>
            → Workers execute operators from different segments of
            a query plan at the same time.<br>
            → More common in streaming systems (continuous
            queries)<br>
            Also called pipeline parallelism.<br>
            <p>Approach #3: Bushy</p><br>
            → Hybrid of intra- and inter-operator
            parallelism where workers execute
            multiple operators from different
            segments of a query plan at the same
            time.<br>
            → Still need exchange operators to
            combine intermediate results from
            segments.<br>
        </div>
        <div class="line"></div>
        <h2>Optimization</h2>
        <div class="text-img-50">
            <img src="imgs/qp.png">
            <div>
            <p>Heuristics / Rules<br>
                → Rewrite the query to remove stupid / inefficient things.<br>
                → These techniques may need to examine catalog, but
                they do not need to examine data.<br>
                Cost-based Search<br>
                → Use a model to estimate the cost of executing a plan.<br>
                → Evaluate multiple equivalent plans for a query and pick
                the one with the lowest cost.</p>
            </div>
            <!-- <img src="imgs/qp.png"> -->
        </div>
        <div class="st">
            <p>一些方法</p><br>
            Relational Algebra Equivalences: 提前project，提前过滤(select)，join拆分(交换律、结合律)<br>
            Logical Query Optimization: 拆分and，过滤(select)提前，join代替cross product，提前project<br>
            Nested Queries: 重写来避开<br>
            Expression Rewriting: 去除明显没用的，用sub-query替换join<br></div>
        <div class="st">
            <p>Cost-Based Search</p><br>
        </div>
        <div class="line"></div>
        <h2>Query Planning</h2>
        <div class="st"><p>Cost-based Search </p><br>
            → Use a model to estimate the cost of executing a plan.<br>
            → Evaluate multiple equivalent plans for a query and pick
            the one with the lowest cost.
        </div>
        <p class="spst">Statistics</p>
        <div class="st">以下无特别说明，¬()表示下标。<br>
            For each relation R, the DBMS maintains the
            following information:<br>
            → <p>N¬(R)</p>: Number of tuples in R. 总数<br>
            → <p>V(A,R)</p>: Number of distinct values for attribute A. 种类数量<br>
            Selection cardinality <p>SC(A,R)=N¬(R)/V(A,R)</p> 平均每种有几个<br>
            The selectivity (<p>sel</p>) of a predicate <p>P</p> is the
            fraction of tuples that qualify. 描述的是能选到的占总数的比例(多少分之一)<br>
            <p>sel(A=constant)=SC(P)/N¬(R)</p><br>
            sel(P1⋀P2) = sel(P1)*sel(P2)<br>
            sel(P1⋁P2) = sel(P1)+sel(P2)–sel(P1)*sel(P2)<br>
            要求P1和P2 independent<br>
        </div>
        <div class="useless">
            General case: R¬(col)s⋂S¬(col)s={A} where A is not a
            primary key for either table.<br>
            → Match each R-tuple with S-tuples:<br>
            estSize ≈ N¬(R) * SC(A, S) = N¬(R)*N¬(S) / V(A,S)<br>
            → Symmetrically, for S:<br>
            estSize ≈ N¬(S) * SC(A, R) = N¬(R)*N¬(S) / V(A,R)<br>
            Overall:<br>
            → estSize ≈ N¬(R) * N¬(S) / max({V(A,S), V(A,R)})
        </div>
        <p>
            Assumptions: Uniform Data, Independent Predicates, Inclusion Principle(the domain of join keys overlap such that each key in
            the inner relation will also exist in the outer table)<br>
        </p>
        <div class="st"><p>其它方式: </p>sampling，sketches(probabilistic data structures that generate
            approximate statistics about a data set. Cost-model can replace histograms with
            sketches to improve its selectivity estimate accuracy.), histogrms(直方图)(可以是按刻度均匀的，也可以是按高度基本相同的，目的是查找一个范围时，可以知道大概情况)</div>
        <p class="spst">Multi-Relation Query Planning</p>
        <div class="text-img-30">
            <div class="st"><p>Left deep: </p>每次合并后的结果都在左边，并合一个原始数据(非合并后结果)合并，正常只需要考虑left deep，因为其它就是交换一下<br>
                1. Enumerate the orderings<br>
                → Example: Left-deep tree #1, Left-deep tree #2…<br>
                2. Enumerate the plans for each operator<br>
                → Example: Hash, Sort-Merge, Nested Loop…<br>
                3. Enumerate the access paths for each table<br>
                → Example: Index #1, Index #2, Seq Scan…<br>
                使用dynamic programming来计算最小的cost。<br>
            </div>
            <img src="imgs/ld.png">
        </div>
        <div class="line"></div>
        <h2>并行处理</h2>
        <div class="st">A <p>transaction</p> is the execution of a sequence of
            one or more operations (e.g., SQL queries) on a
            database to perform some higher-level function.<br>
            A new txn starts with the <p>BEGIN</p> command.<br>
            The txn stops with either <p>COMMIT</p> or <p>ABORT</p>:<br>
            → If commit, the DBMS either saves all the txn's changes.
            If abort, all changes are undone so that it's like as if the
            txn never executed at all.</div>
        <p class="spst">ACID</p>
        <div class="st">
            <p>Atomicity:</p> All actions in the txn happen, or none happen.
            → “all or nothing”<br>
            <p>Consistency:</p> If each txn is consistent and the DB starts
            consistent, then it ends up consistent.
            → “it looks correct to me”<br>
            <p>Isolation:</p> Execution of one txn is isolated from that of
            other txns.
            → “as if alone”<br>
            <p>Durability:</p> If a txn commits, its effects persist.
            → “survive failures”</div>
        <p class="spst">确保Atomicity的方法</p>
        <div class="st"><p>Approach #1: Logging</p><br>
            → DBMS logs all actions so that it can undo the actions of
            aborted transactions.<br>
            → Maintain undo records both in memory and on disk.<br>
            → Think of this like the black box in airplanes…<br>
            Logging is used by almost every DBMS.<br>
            → Audit Trail
            → Efficiency Reasons<br>
            <p>Approach #2: Shadow Paging</p><br>
            → DBMS makes copies of pages and txns make changes to
            those copies. Only when the txn commits is the page
            made visible to others.<br>
            Few systems do this:
            → CouchDB
            → LMDB (OpenLDAP)</div>
        <p class="spst">确保Isolation的方法</p>
        <div class="st">A <p>concurrency</p> control protocol is how the DBMS<br>
            decides the proper interleaving of operations
            from multiple transactions.<br>
            Two categories of protocols:<br>
            → <p>Pessimistic:</p> Don't let problems arise in the first place.<br>
            → <p>Optimistic:</p> Assume conflicts are rare, deal with them
            after they happen.</div>
        <p class="spst">Correctness</p>
        <div class="st">How do we judge whether a schedule is
            correct?<br>
            If the schedule is <p>equivalent</p> to some <p>serial
            execution</p></div>
        <!-- <p class="spst">Conflicting Operations</p>
        <div class="st">Two operations <p>conflict</p> if:<br>
            → They are by different transactions,<br>
            → They are on the same object and at least one of them
            is a write.<br>
            Read-Write Conflicts (R-W) |
            Write-Read Conflicts (W-R) |
            Write-Write Conflicts (W-W)</div> -->
        <p class="spst">Conflict Serializable Types</p>
        <div class="st">Two schedules are <p>conflict equivalent</p> iff:<br>
            → They involve the same actions of the same
            transactions, and<br>
            → Every pair of conflicting actions is ordered the same
            way.<br>
            Schedule S is <p>conflict serializable</p> if:<br>
            → S is conflict equivalent to some serial schedule.<br>
            注意这里如果讨论COMMIT，所有的读取都是认为读取了已经写入的（而不是没有commit的保持原样）。另外，一般在讨论是否相等的时候，不考虑abort，有commit就认为一定会commit。这里commit的作用只是标记结束，而不是实际保存数据。<br>
            <p>View Serializability</p>: weaker than conflict Serializability, 允许更多的schedule，但是执行难度更大，实际没有数据库这样做。<br>
            Schedules S1
            and S2
            are view equivalent if:<br>
            → If T1 reads initial value of A in S1, then T1
            also reads initial value of A in S2.<br>
            → If T1 reads value of A written by T2
            in S1, then T1 also reads value of A written by T2 in S2.<br>
            → If T1 writes final value of A in S1, then T1
            also writes final value of A in S2.<br>
        </div>
        <div class="text-img-32">
            <div class="st"><p>Dependency Graph</p><br>
                每个transaction有一个圈，箭头代表哪个操作conflict，从先执行的指向后执行的，被依赖的指向依赖的(和常识反过来)，箭头上可以写变量的名称<br>
                graph acyclic就代表conflict serializable，否则不行<br>
                Also known as a precedence graph.
            </div>
            <img src="imgs/dg.png">
        </div>
        <div class="line"></div>
        <h2>Two-Phase Lock</h2>
        <div class="st">
            <p>S-LOCK</p>: Shared locks for reads. 只要是只有读取就申请这个，多个可以同时持有。<br>
            <p>X-LOCK</p>: Exclusive locks for writes. 写入必须申请这个，不能同时持有，和shared lock同时都不行。</div>
        <div class="st"><p>Phase #1: Growing</p><br>
            → Each txn requests the locks that it needs from the
            DBMS’s lock manager.<br>
            → The lock manager grants/denies lock requests.<br>
            <p>Phase #2: Shrinking</p><br>
            → The txn is allowed to only release locks that it
            previously acquired. It cannot acquire new locks.<br>
            2PL on its own is sufficient to guarantee conflict
            serializability.<br>
            But it is subject to cascading aborts. 即如果考虑abort不能保证</div>
        <p class="spst">Strong Strict 2PL (Rigorous 2PL)</p>
        <div class="st">
            grow阶段要求相同，但是shrink只能在结束后(commit或abort后)释放，在这之前必须一直占据lock，保持相同。<br>
            可以在有abort的情况下保证conflict serializability。<br>
            有可能导致deadlock
        </div>
        <p class="spst">Deadlock</p>
        <p>Waits-for graph, 箭头方向和dependency graph相反。</p>
        <div class="text-img-100">
            <img src="imgs/dl.png">
        </div>
        <div class="st">
            <p>Approach #1: Deadlock Detection</p><br>
            The DBMS creates a waits-for graph to keep
            track of what locks each txn is waiting to acquire:<br>
            The system periodically checks for cycles in
            waits-for graph and then decides how to break
            it.<br>
            当检测到时，可以选一个abort(更多)或restart，victim的选取可以根据开始时间、进度、占用资源、修改资源等。<br>
            <p>Approach #2: Deadlock Prevention</p><br>
            When a txn tries to acquire a lock that is held by
            another txn, the DBMS kills one of them to
            prevent a deadlock.<br>
            This approach does not require a waits-for graph
            or detection algorithm.<br>
            Older Timestamp = Higher Priority<br>
            1. Wait-Die ("Old Waits for Young")<br>
            → If requesting txn has higher priority than holding txn,
            then requesting txn waits for holding txn.
            → Otherwise requesting txn aborts.<br>
            2. Wound-Wait ("Young Waits for Old")<br>
            → If requesting txn has higher priority than holding txn,
            then holding txn aborts and releases lock.
            → Otherwise requesting txn waits.</div>
        <div class="st"><p>Weaker Level of Isolations</p>: Serializability is useful because it allows
            programmers to ignore concurrency issues. But enforcing it may allow too little concurrency
            and limit performance.</div>
        <p><table>
            <tr>
                <th>Transaction Isolation Level</th>
                <th>Dirty Read</th>
                <th>Unrepeatable Read</th>
                <th>Phantom</th>
            </tr>
            <tr>
                <td>Serializable</td>
                <td class="no">No</td>
                <td class="no">No</td>
                <td class="no">No</td>
            </tr>
            <tr>
                <td>Repeatable Read</td>
                <td class="no">No</td>
                <td class="no">No</td>
                <td class="maybe">Maybe</td>
            </tr>
            <tr>
                <td>Read Committed</td>
                <td class="no">No</td>
                <td class="maybe">Maybe</td>
                <td class="maybe">Maybe</td>
            </tr>
            <tr>
                <td>Read Uncommitted</td>
                <td class="maybe">Maybe</td>
                <td class="maybe">Maybe</td>
                <td class="maybe">Maybe</td>
            </tr>
        </table></p>
        <div class="line"></div>
        <h2>Logging</h2>
        <div class="st">
            <p>Undo</p>: The process of removing the effects of an
            incomplete or aborted txn.<br>
            <p>Redo</p>: The process of re-instating the effects of a
            committed txn for durability. 主要针对的是commit了(但没有txn-end)但是没有彻底写入更改的情况，和checkpoint之后有更改且commit了。<br>
            <p>Steal</p>: allows an uncommitted txn
            to overwrite the most recent committed value of
            an object in non-volatile storage.<br>
            <p>No-steal</p>: 不允许这种。<br>
            <p>Force</p>:  requires that all updates
            made by a txn are reflected on non-volatile
            storage before the txn can commit.<br>
            <p>No-force</p>: 对此没有要求。
        </div>
        <p>普通的no-steal+force：不需要redo、undo (假设[大段]写入atomic)</p>
        <p>Runtime Performance: steal+no-force fastest, no-steal+force slowest</p>
        <p>Runtime Performance: 两种相反</p>
        <p class="spst">Shadow Paging</p>
        <div class="st">Maintain two separate copies of the database:<br>
            → <p>Master</p>: Contains only changes from committed txns.<br>
            → <p>Shadow</p>: Temporary database with changes made from
            uncommitted txns.<br>
            When a txn commits, atomically switch the
            shadow to become the new master.<br>
            Buffer Pool Policy: NO-STEAL + FORCE<br>
            应用更改只需要修改root。不需要redo，undo只需要删除shadow pages。
        </div>
        <p class="spst">Journal File (SQLITE 2010之前)</p>
        <p>When a txn modifies a page, the
            DBMS copies the original page to a
            separate journal file before
            overwriting master version.<br>
            After restarting, if a journal file
            exists, then the DBMS restores it to
            undo changes from uncommitted
            txns.</p>
        <div class="text-img-25">
            <div>
                <p class="spst">Write-Ahead Log (WAL)</p>
                <p>Maintain a log file separate from data files that
                    contains the changes that txns make to database.<br>
                    → Assume that the log is on stable storage.<br>
                    → Log contains enough information to perform the
                    necessary undo and redo actions to restore the database.<br>
                    DBMS must write to disk the log file records that
                    correspond to changes made to a database object
                    before it can flush that object to disk.<br>
                    Buffer Pool Policy: STEAL + NO-FORCE<br>
                </p>
            </div>
            <img src="imgs/wal2.png">
        </div>
        <p>在实际执行时，只先写入内存，不一定每一步都写入硬盘。commit时或累积到一定数量时会写入硬盘。但是commit时必须把它之前的全部写入，只有所有相关的都写入了才视为commit成功。</p>
        <div class="st">
            1. 在开始时写入 Tid BEGIN<br>
            2. 每一步写入 Tid, object_id, before_value(undo), after_value(redo)<br>
            3. 结束时写入 Tid COMMIT/ABORT<br>
        </div>
        <p>Physical Logging: the location of changes in the
            database<br>
            Logical Logging: high-level operations, UPDATE DELETE, etc.<br>
            Physiological: 两种结合
        </p>
        <p class="spst">Checkpoints</p>
        <p>写入checkpoint的地方代表前面的都写入了储存区，后面的没有，crash的时候可以快速恢复。可以在txn中间进行。<br>
            crash后对后面的更改，进行对应的undo redo。是steal+no-force。这里的checkpoint考虑阻塞所有操作。
        </p>
        <div class="line"></div>
        <h2>Recovery</h2>
        <div class="st"><p>ARIES</p>: Algorithms for Recovery and
            Isolation Exploiting Semantics
        </div>
        <p>Write-Ahead Logging<br>
            Repeating History During Redo:
            → On restart, retrace actions and restore database to
            exact state before crash.<br>
            Logging Changes During Undo:
            → Record undo actions to log to ensure action is not
            repeated in the event of repeated failures.<br></p>
        <p class="spst">Log Sequence Numbers</p>
        <div class="st">
            1. <p>flushedLSN</p>: Last LSN in log on disk, in memory<br>
            2. <p>pageLSN</p>: Newest update to page(x), in page(x). 这个page最新的记录，每个page都有1个。每次这个page有新的改动，都必须更新。<br>
            3. <p>recLSN</p>: Oldest update to page(x) since it was last flushed, in page(x). 这个page在flush之后的第一个记录，每个page都有1个。<br>
            4. <p>lastLSN</p>: Latest record of txn T(i), in T(i). 每个txn有一个。<br>
            5. <p>MasterRecord</p>: LSN of latest checkpoint, in disk<br>
            Before page x can be written to disk, we must
            flush log at least to the point where:
            → pageLSN(x) <= flushedLSN，这里的意思是说，要在这个page的记录都在硬盘上(而不是内存)之后，才可以实际写入储存区。<br>
            写入COMMIT记录时，它和它之前的必须写入硬盘，因为返回了就必须成功。
        </div>
        <div class="st"><p>更新：</p>
            在结束(不管是commit还是abort[但是abort要先undo])之后，写入TXN-END，才算实际结束。COMMIT了没有TXN-END同样要undo(虽然我不知道这有什么实际意义，理论上txn-end应该是紧跟在commit之后的，但是有的时候会有间隔)。<br>
        </div>
        <div class="text-img-100">
            <div><p>在这张图中，MasterRecord是007，flushedLSN是016，pageLSN是019。</p></div>
            <img src="imgs/lsn.png">
        </div>
        <p>在这张图中，MasterRecord是007，flushedLSN是016，pageLSN是019。</p>
        <p class="spst">Abort / Undo</p>
        <div class="st"><p>prevLSN</p>: 在LSN列加入prevLSN，在undo时方便查找那个txn之前的记录。nil表示begin。</div>
        <div class="text-img-70">
            <div>
                <img src="imgs/prev.png" width="100%" style="vertical-align: top;">
                从那个txn的最后一个记录开始，进行CLR，交换before和after的值，然后依次往前。undoNext记录它的prev，之后的可以直接根据这个找到前一个。<br>
            </div>
            <img src="imgs/undo.png" width="70%" style="display:inline"><br>
        </div>
        <div class="st">
            First write an ABORT record to log for the txn.<br>
            Then play back the txn's updates in reverse
            order. For each update record:<br>
            → Write a CLR entry to the log.
            → Restore old value.<br>
            At end, write a TXN-END log record.<br>
            Notice: CLRs never need to be undone.
        </div>
        <div class="st"><p>Slightly better checkpoint: </p>在checkpoint中记录ATT和DPT，record继续，不阻塞。注意这里还没有begin end。这里不做过多介绍。</div>
        <p class="spst">Fuzzy checkpoint</p>
        <div class="st"><p>Checkpoint-begin</p>: 开始。这时log可以继续更新<br>
            <p>Checkpoint-end</p>: 结束，包含ATT(Active Txn Table)和DPT(Dependent Page Table)。<br>
            ATT包含Tid，status，lastLSN。DPT包含pageId，recLSN。<br>
            ATT的status有 R,C,U (R)unning, (C)ommitting, U(candidate for Undo)<br>
            checkpoint begin之后的不会被包含在ATT和DPT中，它描述的是begin时的状态。MasterRecord指向begin<br>
            checkpoint时不会把ATT和DPT中的更新写入储存区。
        </div>
        <p class="spst">Recovery Process (ARIES)</p>
        <div class="st"><p>Step #1 – Analysis</p><br>
            根据MasterRecord找到checkpoint begin，从那里开始，模拟后面的任务。先把ATT中的所有都改成U。有txn-end，从ATT中删除，有commit，ATT中改成C，有update(begin)，加入ATT(为U)，若它不在DPT中，加入DPT(除了begin)，recLSN为那个记录的LSN。结束之后会有新的ATT和DPT。注意这时只更新ATT和DPT，不会实际向储存区写入任何数据。<br>
            <p>Step #2 – Redo</p><br>
            退回到DPT中最小的recLSN的record，从那里开始执行所有的record(写入储存区操作)(包括CLR)，除非它不在DPT中，或它的LSN小于DPT中该page的recLSN。
            这里的意思是不管怎样都要写入(即使要abort或undo)，那两个例外情况只是因为已经写进去了。最后对ATT中commited的写入txn-end，并从ATT中删除。<br>
            <p>Step #3 – Undo</p><br>
            对没有txn-end(或committed)(即这时仍在ATT中的)的进行undo，按record顺序从后往前CLR。<br>
            <p>综上</p>，analysis的目的是找到需要redo和undo的部分，DPT的都要redo(即使要clr或abort)，目的是确保不管怎样log全部写入储存区，ATT的(除了C)都要undo，目的是没有完成的或abort的全部恢复原样。
        </div>
        <p>What does the DBMS do if it crashes during
            recovery in the Analysis Phase?<br>
            → Nothing. Just run recovery again.<br>
            What does the DBMS do if it crashes during
            recovery in the Redo Phase?<br>
            → Again nothing. Redo everything again.</p>
        <div class="line"></div>
        <h2 id="name">Tiancheng Jiao&nbsp;|&nbsp;tcjiao&nbsp;|&nbsp;EECS 484</h2>
    </div>
    <!-- 
        <div class="st"><p></p></div>
        <p class="spst"></p>
        <div class="st"></div>
     -->
     <footer></footer>
</body>

</html>